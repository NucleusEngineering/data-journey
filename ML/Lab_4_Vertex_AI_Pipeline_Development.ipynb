{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a6a6b2-43f7-449f-94a2-619e4daa8f11",
      "metadata": {
        "id": "e0a6a6b2-43f7-449f-94a2-619e4daa8f11"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d6471c-69ba-4987-933f-e0e2eac2b42a",
      "metadata": {
        "id": "e3d6471c-69ba-4987-933f-e0e2eac2b42a"
      },
      "source": [
        "# **Lab 4:** Vertex AI Pipeline Development\n",
        "This lab orchestrates the content Labs 2 & 3 into a vertex pipeline. In our pipeline we will use supported operators to:\n",
        "* **BigqueryCreateModelJobOp**: To train a logistic regression model\n",
        "* **BigqueryPredictModelJobOp**: To run our predictions and save the results to a BQ table\n",
        "* **BigqueryExportModelJobOp**, **importer_node**, **ModelUploadOp**: To export our BQML model in tensorflow format in GCS and upload to vertex model registry\n",
        "* **EndpointCreateOp**: To create our endpoint\n",
        "* **ModelDeployOp**: To deploy our registered model to our endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab7758b-d973-4ad1-b2c5-923216123772",
      "metadata": {
        "id": "aab7758b-d973-4ad1-b2c5-923216123772"
      },
      "source": [
        "![overview](assets/pipeline.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9631f759-7352-4940-94be-97e605ad11b3",
      "metadata": {
        "id": "9631f759-7352-4940-94be-97e605ad11b3"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade \"kfp\" \\\n",
        "                         \"google-cloud-aiplatform\" \\\n",
        "                         \"google-cloud-storage\" \\\n",
        "                         \"google_cloud_pipeline_components\" \\\n",
        "                         \"google-cloud-bigquery\" --user -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2aa8a5d-9c97-47c7-b34c-392f8725a787",
      "metadata": {
        "id": "f2aa8a5d-9c97-47c7-b34c-392f8725a787"
      },
      "outputs": [],
      "source": [
        "project_id = \"<project-id>\"\n",
        "location     = \"us\"\n",
        "region       = \"us-central1\"\n",
        "team_name    = \"<team name>\"\n",
        "dataset_name = \"datathon_ds_{}\".format(team_name)\n",
        "bucket_name  = \"gs://{}_{}\".format(project_id,dataset_name)\n",
        "pipeline_root_path = bucket_name + '/pipelines/'\n",
        "model_artificat_path = bucket_name + '/vertex_pipelines_models'\n",
        "model_artificat_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78164d6-35e0-4e77-b480-3798fd94e384",
      "metadata": {
        "id": "a78164d6-35e0-4e77-b480-3798fd94e384"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $project_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee1e3633-603e-49ce-9dcb-db236f97564d",
      "metadata": {
        "id": "ee1e3633-603e-49ce-9dcb-db236f97564d"
      },
      "source": [
        "We need to restart the kernel to make sure we reference the newly installed libraries\n",
        "* **Kernel** -> **Restart Kernel and clear outputs**\n",
        "\n",
        "Execute the commands from the first cell again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d42428-b123-4a19-bc2d-899441cddaa0",
      "metadata": {
        "id": "b0d42428-b123-4a19-bc2d-899441cddaa0"
      },
      "outputs": [],
      "source": [
        "from kfp import compiler, dsl\n",
        "from kfp.dsl import importer_node\n",
        "from kfp.dsl import HTML, Artifact, Condition, Input, Output, component\n",
        "from google_cloud_pipeline_components.v1.bigquery import (\n",
        "    BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp, BigqueryExportModelJobOp,\n",
        "    BigqueryExplainForecastModelJobOp, BigqueryForecastModelJobOp,BigqueryMLConfusionMatrixJobOp, BigqueryPredictModelJobOp,\n",
        "    BigqueryMLArimaEvaluateJobOp, BigqueryQueryJobOp)\n",
        "from google.cloud import aiplatform\n",
        "from google_cloud_pipeline_components.types import artifact_types\n",
        "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
        "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp, ModelDeployOp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4071464-7818-4bcc-a0d7-15c168158e50",
      "metadata": {
        "id": "a4071464-7818-4bcc-a0d7-15c168158e50"
      },
      "outputs": [],
      "source": [
        "# Define the workflow of the pipeline.\n",
        "@dsl.pipeline(\n",
        "    name=\"bqml-pipeline\",\n",
        "    pipeline_root=pipeline_root_path)\n",
        "\n",
        "def pipeline(\n",
        "    project: str,\n",
        "    dataset: str,\n",
        "    location: str,\n",
        "    region: str\n",
        "):\n",
        "    # Learn more about operators used here -> https://cloud.google.com/vertex-ai/docs/pipelines/bigqueryml-component\n",
        "    model_create = BigqueryCreateModelJobOp(\n",
        "        project=project,\n",
        "        location=location,\n",
        "        query=f\"\"\"\n",
        "        CREATE OR REPLACE MODEL `{project}.{dataset}.vertex_pipeline_logistic_regression_baseline`\n",
        "        OPTIONS(MODEL_TYPE='LOGISTIC_REG',\n",
        "                INPUT_LABEL_COLS = ['churned'])\n",
        "        AS\n",
        "        SELECT * EXCEPT (user_pseudo_id)\n",
        "        FROM `{project}.{dataset}.cc_train_dataset`\n",
        "        \"\"\"\n",
        "        ).set_display_name(\"Train logistic regression baseline\")\n",
        "\n",
        "    _ = BigqueryPredictModelJobOp(\n",
        "        project=project,\n",
        "        location=location,\n",
        "        model=model_create.outputs[\"model\"],\n",
        "        query_statement=f'''SELECT * EXCEPT(user_pseudo_id, churned)\n",
        "                            FROM\n",
        "                            `{project}.{dataset}.cc_eval_dataset`\n",
        "                            ''',\n",
        "        job_configuration_query={\n",
        "            \"destinationTable\": {\n",
        "                \"projectId\": project_id, \n",
        "                \"datasetId\": dataset_name, \n",
        "                \"tableId\": \"results_1\", # change table for every new run\n",
        "            }\n",
        "        },\n",
        "        ).set_display_name(\"Prediction on evaluation set\").after(model_create)\n",
        "\n",
        "    bq_export = BigqueryExportModelJobOp(\n",
        "        project=project,\n",
        "        location=location,\n",
        "        model=model_create.outputs[\"model\"],\n",
        "        model_destination_path=model_artificat_path,\n",
        "    ).set_display_name(\"Export BQ model to GCS\").after(model_create)\n",
        "\n",
        "    import_unmanaged_model_task = importer_node.importer(\n",
        "        artifact_uri=model_artificat_path,\n",
        "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
        "        metadata={\n",
        "            \"containerSpec\": {\n",
        "                \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\",\n",
        "            },\n",
        "        },\n",
        "    ).after(bq_export)\n",
        "\n",
        "    model_upload = ModelUploadOp(\n",
        "        project=project,\n",
        "        display_name=\"vertex_pipeline_model_logistic_regression\",\n",
        "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
        "    ).after(import_unmanaged_model_task)\n",
        "\n",
        "    endpoint = EndpointCreateOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        display_name=\"vertex_pipeline_deployment\",\n",
        "    ).after(model_upload)\n",
        "\n",
        "    _ = ModelDeployOp(\n",
        "        model=model_upload.outputs[\"model\"],\n",
        "        endpoint=endpoint.outputs[\"endpoint\"],\n",
        "        dedicated_resources_min_replica_count=1,\n",
        "        dedicated_resources_max_replica_count=1,\n",
        "        dedicated_resources_machine_type='n1-standard-2',\n",
        "        traffic_split={\"0\": 100},\n",
        "    ).set_display_name(\"Deploy to endpoint\").after(endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e159558d-93ae-4705-b8e8-e3c0db40309d",
      "metadata": {
        "id": "e159558d-93ae-4705-b8e8-e3c0db40309d"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(pipeline_func=pipeline,\n",
        "        package_path='churn_prediction_pipeline.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "789a94b0-6a23-4554-b7fe-3a281408df6d",
      "metadata": {
        "id": "789a94b0-6a23-4554-b7fe-3a281408df6d"
      },
      "outputs": [],
      "source": [
        "# Prepare the pipeline job\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=\"bqml-vertex-pipeline1\",\n",
        "    template_path=\"churn_prediction_pipeline.json\",\n",
        "    pipeline_root=pipeline_root_path,\n",
        "    enable_caching=True,\n",
        "    parameter_values={\n",
        "        'project': project_id,\n",
        "        'dataset': dataset_name,\n",
        "        'location': location,\n",
        "        'region': region\n",
        "    }\n",
        ")\n",
        "\n",
        "job.submit()"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "toc-autonumbering": true,
    "toc-showcode": false,
    "toc-showmarkdowntxt": true,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
